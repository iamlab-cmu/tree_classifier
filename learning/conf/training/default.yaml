# Training configuration
epochs: 25
batch_size: 4
learning_rate: 3e-5
weight_decay: 0.02
lr_factor: 0.5
scheduler_patience: 2
early_stopping_patience: 3
num_workers: 2
seed: 42

# Loss function
loss_function: "cross_entropy"

# Optimizer
optimizer: "adamw"
